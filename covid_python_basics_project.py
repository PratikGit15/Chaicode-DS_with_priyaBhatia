# -*- coding: utf-8 -*-
"""COVID_python_basics_project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jOfLsyabBVQDKR5KTAiwL3sfBHDbxvzQ

Que: Does ML model draw inferences from categorical data also, like gender? If then then categorical data also should be correctly imputed.

# *Problem Statement & Importing necessary libraries*

Background

The COVID-19 pandemic, caused by the SARS-CoV-2 virus, emerged in late 2019 and rapidly spread globally, leading to significant health, economic, and social impacts. This unprecedented health crisis highlighted the crucial role of data analysis in managing such pandemics. By meticulously tracking and analyzing data on confirmed cases, recoveries, and deaths, policymakers and health professionals can make informed decisions to control the spread of the virus and allocate resources effectively.

Dataset Details:

This case study utilizes three key datasets, each providing daily updates on different aspects of the pandemic for various countries and regions:

*   Confirmed Cases Dataset: This dataset contains the cumulative number of confirmed COVID-19 cases per day for each country and region. The data spans from January 22, 2020, to May 29, 2021, with over 276 geographic entries.
*   Deaths Dataset: This dataset records the cumulative number of deaths attributed to COVID-19, structured similarly to the confirmed cases dataset. It provides crucial information for assessing the lethality and outbreak severity in different areas.
*   Recovered Cases Dataset: Includes data on the cumulative number of individuals who have recovered from COVID-19, which is vital for understanding the disease's progression and the effectiveness of treatment protocols.

Each dataset includes columns for Province/State, Country/Region, geographic coordinates (Lat, Long), and a series of dates representing daily cumulative totals.

*Importing required modules.*
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""*Mounting G-drive. Or comment it if connected.*"""

#from google.colab import drive
#drive.mount('/content/drive')

"""# **Analysis of confirmed_case_data**

*Importing datasets. Provide the correct path file as per your system.*
"""

confirmed_case_data = pd.read_csv('/content/drive/MyDrive/ChaiCode/covid_19_confirmed_v1_lyst1747728690432.csv')

"""*Take a glance at dataset one by one.*"""

print(f'The shape of the confirmed_cases_data set is {confirmed_case_data.shape}', end = '\n\n')
confirmed_case_data.head(3)

"""*Refactoring the column positions and names for easyness.*

*   *Swapping Country & province column; country being the broader category.*
*   *Full name of Lat & Long.*
*   *Formatting date coluns as per IST standards.*
"""

#converting dataframe columns to list columns to do swap
col_list = list(confirmed_case_data.columns)
col_list[0], col_list[1] = col_list[1], col_list[0]
# assigning column list back to dataframe
confirmed_case_data = confirmed_case_data[col_list]

confirmed_case_data.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)

#To convert them we need to parse them as date, then only we can reformat them.
#It also ensures all dates are correctly interpreted and formatted, especially if months/days are single digits.
date_columns = confirmed_case_data.columns[4:]
new_date_columns = pd.to_datetime(date_columns, format='%m/%d/%y').strftime('%d/%b/%Y') ## strftime convert datetime object to a string in a specified format.
confirmed_case_data.columns = list(confirmed_case_data.columns[:4]) + list(new_date_columns)

confirmed_case_data.head(3)

"""*Now lets see dtypes of each column.*"""

confirmed_case_data.info()  # usually used to see dtypes. Pandas is truncating the output when there are too many columns.

confirmed_case_data.dtypes
# Date dtypes shuold be int64 as date is never in decimal.
# But if there are any missing values in date column, then its stype would be float64 (since pandas uses NaN for missing values, which is a float).

"""*From above we can see dtypes of Country, Province, Latitude and Longitude are fine.*

Let's check all Date column dtypes.
"""

print( * (col for col in confirmed_case_data.columns[4:] if 'int' not in str(confirmed_case_data[col].dtype)))
# unpacking operator * used to print the column names, otherwise it would print generator object.

# date_columns = confirmed_case_data.columns[4:]
# confirmed_case_data[date_columns] = confirmed_case_data[date_columns].astype('int64')

"""*Handling missing values*"""

# confirmed_case_data.isnull().sum() will give truncated result.
print(confirmed_case_data.isnull().any().sum()) # returns series with column name as index and value is the count of missing values in that column.

"""`// Don't forget to do isna() check at last`"""

# printing column names containing NaN values.
null_col = confirmed_case_data.isnull().sum()
print(null_col [null_col>0].to_string()) # to print where null values is more than 0.
# printing a pandas Series prints the dtype also at the end of the Series output (e.g., dtype: int64)
# Using to_string() converts series into its string representation for display purposes.

"""Above we have checked for NaN values. Note that it doesn't check for 0 values.

**Imputing Province missing values:**

*What to impute for Province ?*


*   From below we can see that 1 country has N provinces but the province is not repeated. Meaning each row is unique for a particular geolocation. So, we cannot use mode or median for imputation because it will lead to duplicate location. Hence we will impute the missing province values with 'Unknown' or could use the Country name itself in province.
{for those whose latit & longi are given, we could search the province from web, but not done as we have their co-ordinates anyway and province is just a categorical data}
"""

# Cross check if 1 country has many provinces.
duplicate_countries = confirmed_case_data[confirmed_case_data.duplicated(subset = ['Country/Region'])]
print(duplicate_countries.shape, end = '\n\n')
duplicate_countries

# Checking if there are repeated Country-Province pair.
duplicate_country_province = confirmed_case_data[confirmed_case_data.duplicated(subset = ['Country/Region', 'Province/State'])]
duplicate_country_province

"""Imputing 'All Provinces' value for missing province."""

confirmed_case_data['Province/State'] = confirmed_case_data['Province/State'].fillna('All Provinces')
confirmed_case_data.head(3)

"""Cross checking if it is updated"""

null_col = confirmed_case_data.isnull().sum()
print(null_col [null_col>0].to_string())
# So it is only Latitude and Longitude where values are NaN.

"""**Imputing missing values of Latitudes & Longitudes:**

*It is ideal that since dataset is geolocation specific, latitudes and longitudes should also be not repeated. But lets take a look.*
"""

duplicate_lat_long = confirmed_case_data[confirmed_case_data.duplicated(subset = ['Latitude', 'Longitude'])]
duplicate_lat_long

"""*   From above we can see latitudes & longitudes are not repeated.
*   There are some (0,0) values which is humorously referred to as "Null Island" in the Atlantic Ocean where there is is no land, which also needs to be imputed.

*Lets see what all rows has NaN or 0 as Latitude or Longitudes.*
"""

missing_coords = confirmed_case_data[
    confirmed_case_data['Latitude'].isnull() |
    (confirmed_case_data['Latitude'] == 0) |
    confirmed_case_data['Longitude'].isnull() |
    (confirmed_case_data['Longitude'] == 0)
]
print(missing_coords.to_string())

for country in ['Canada', 'China', 'Diamond Princess', 'MS Zaandam']:
  print(country, confirmed_case_data.loc[confirmed_case_data['Country/Region'] == country].shape)

"""***What to impute in Latitudes and Longitudes?***

Latitude and longitudes should be treated as categorical or descrete or continous data ?

*   We cannot use mode or median, as it will lead to duplicate coordinates and may not represent that specific province of that country.

*   Observing filled data we saw coordinates represent the approximate central points of the capital cities of those provinces. For correct country and province, we can find the coordinates from internet. But there could be N such rows in large dataset. We will impute with the mean of that country's provinces coordinates. Logically coordinates are categorical data as here they are representing unique location, but given as continous numerical data.

 Here Diamond & Grand Princess are cruise ships quaratined during covid. And repatriated represents returned citizens, not a location. But dataset can cotain more such rows with filled Coordinates which we have not figured out. That's why we imputed these rows with mean of that country.

*   Diamond cruise and MS Zaandam in country column are cruise ship with province Unknown. And there is also 1 entry of them so cannot impute them with mean also. For not to loose data, we imputed them with 180 (which is pacific ocean loc) and document the value as special case.

Imputing Latitude and Longitude:
"""

for country in ['Canada', 'China']:
    country_specific_df = confirmed_case_data['Country/Region'] == country
    # Calculate mean for non-zero, non-NaN latitudes and longitudes
    latitude_mean = confirmed_case_data.loc[country_specific_df & (confirmed_case_data['Latitude'] != 0), 'Latitude'].mean()
    longitude_mean = confirmed_case_data.loc[country_specific_df & (confirmed_case_data['Longitude'] != 0), 'Longitude'].mean()
    # Replace NaN or 0 with the mean
    empty_latitude_column = country_specific_df & (confirmed_case_data['Latitude'].isnull() | (confirmed_case_data['Latitude'] == 0))
    empty_longitude_column = country_specific_df & (confirmed_case_data['Longitude'].isnull() | (confirmed_case_data['Longitude'] == 0))
    confirmed_case_data.loc[empty_latitude_column, 'Latitude'] = latitude_mean
    confirmed_case_data.loc[empty_longitude_column, 'Longitude'] = longitude_mean

"""Cross check if NaN is updated with Mean."""

confirmed_case_data.loc[[41, 42, 52, 58]]

# to update specific latitude and longitude
confirmed_case_data.loc[105, ['Latitude', 'Longitude']] = [180, -180]
confirmed_case_data.loc[173, ['Latitude', 'Longitude']] = [180, -180]
confirmed_case_data.loc[[105, 173]] # loc prints the row as a Series by default. We have printed it as DataFrame row using [[]]

"""Cross check if any zero or NaN coordinates"""

missing_coords = confirmed_case_data[
    confirmed_case_data['Latitude'].isnull() |
    (confirmed_case_data['Latitude'] == 0) |
    confirmed_case_data['Longitude'].isnull() |
    (confirmed_case_data['Longitude'] == 0)
]
if missing_coords.empty:
    print("No missing or zero coordinates found.")
else:
    print(missing_coords.to_string())

"""# **Analysis of death_data**

*Loading of dataset*
"""

death_data = pd.read_csv('/content/drive/MyDrive/ChaiCode/covid_19_deaths_v1_lyst1747728711771.csv')

"""*Data at glance*"""

print(f'The shape of the death_data set is {death_data.shape}', end = '\n\n')
death_data.head(3)

"""Here you note that first 0th row is headers, so the actual datashape is not 277x498 but it is 276x498.

*Changed column names for easy understanding.*
"""

# Reset the column names using the first row
death_data.columns = death_data.iloc[0]
# Drop the first row since it's now the header
death_data = death_data[1:].reset_index(drop=True) # drop true means old index is discarded and not added as a new column in your DataFram

#converting dataframe columns to list columns to do swap
col_list = list(death_data.columns)
col_list[0], col_list[1] = col_list[1], col_list[0]
# assigning column list back to dataframe
death_data = death_data[col_list]

death_data.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)

#To convert them we need to parse them as date, then only we can reformat them.
#It also ensures all dates are correctly interpreted and formatted, especially if months/days are single digits.
date_columns = death_data.columns[4:]
new_date_columns = pd.to_datetime(date_columns, format='%m/%d/%y').strftime('%d/%b/%Y') ## strftime convert datetime object to a string in a specified format.
death_data.columns = list(death_data.columns[:4]) + list(new_date_columns)

death_data.head(3)

"""*Correcting dtype of columns*"""

death_data.dtypes

"""*Converting latitude and longitude dtype to float*"""

death_data['Longitude'] = death_data['Longitude'].astype('float64')
death_data['Latitude'] = death_data['Latitude'].astype('float64')

"""*Checking if any date column has dtype other than object*"""

print( * (col for col in death_data.columns[4:] if 'object' not in str(death_data[col].dtype)))

"""*Converting date column from object to int64*"""

# date_columns = death_data.columns[4:]
# death_data[date_columns] = death_data[date_columns].astype('int64')  # GIVES ValueError: cannot convert float NaN to integer
# death_data.dtypes

# we cannot change dtype of Date columns because one of the date column has NaN value. Nan is a float value, not int.
# so before converting dtype, we need to change NaN (float) with any int value. Then we can convert dtype to int.

"""*Columns with NaN values*"""

print(f' Number of Columns containing NaN values = {death_data.isnull().any().sum()}', end = '\n\n')

null_col = death_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""*Imputing provinces with value 'All Provinces'*"""

death_data['Province/State'] = death_data['Province/State'].fillna('All Provinces')
death_data.head(3)

"""Checking which rows has null or 0 latitudes and longitudes"""

missing_coords = death_data[
    death_data['Latitude'].isnull() |
    (death_data['Latitude'] == 0) |
    death_data['Longitude'].isnull() |
    (death_data['Longitude'] == 0)
]
print(missing_coords.to_string())

"""*We can see the situation is same as first dataset.*

*Imputing Latitudes & Longitudes.*
"""

for country in ['Canada', 'China']:
    country_specific_df = death_data['Country/Region'] == country
    # Calculate mean for non-zero, non-NaN latitudes and longitudes
    latitude_mean = death_data.loc[country_specific_df & (death_data['Latitude'] != 0), 'Latitude'].mean()
    longitude_mean = death_data.loc[country_specific_df & (death_data['Longitude'] != 0), 'Longitude'].mean()
    # Replace NaN or 0 with the mean
    empty_latitude_column = country_specific_df & (death_data['Latitude'].isnull() | (death_data['Latitude'] == 0))
    empty_longitude_column = country_specific_df & (death_data['Longitude'].isnull() | (death_data['Longitude'] == 0))
    death_data.loc[empty_latitude_column, 'Latitude'] = latitude_mean
    death_data.loc[empty_longitude_column, 'Longitude'] = longitude_mean

# to update specific latitude and longitude
death_data.loc[105, ['Latitude', 'Longitude']] = [180, -180]
death_data.loc[173, ['Latitude', 'Longitude']] = [180, -180]

missing_coords = death_data[
    death_data['Latitude'].isnull() |
    (death_data['Latitude'] == 0) |
    death_data['Longitude'].isnull() |
    (death_data['Longitude'] == 0)
]
if missing_coords.empty:
    print("No missing or zero coordinates found.")
else:
    print(missing_coords.to_string())

death_data.loc[[41, 42, 52, 88, 105, 173]] # loc prints the row as a Series by default. We have printed it as DataFrame row using [[]]

"""*Impting column '20/Apr/2020' Nan value.*

Lets see how many rows of column '20/Apr/2025' has NaN value.
"""

nan_rows = death_data[death_data['20/Apr/2020'].isnull()] # returns dataframe of all rows with NaN in column 20/Apr/2020
# print(nan_rows.to_string()) # -> df
print(nan_rows.index, end = '\n\n') # returns indexes of rows having NaN values -> Index (labels to access values in series or df). print(nan_rows.index[0])
death_data.loc[nan_rows.index, '20/Apr/2020'] # .loc[row label, column lablel] -> series (1D labelled array and that single column index will be 0)

"""*We can impute it with previous or forward day data, but lets see both days data first.*"""

previous_day_death = int(death_data.loc[nan_rows.index, '19/Apr/2020'].values[0])
one_day_after_death = int(death_data.loc[nan_rows.index, '21/Apr/2020'].values[0])

print(f'Previous day death = {previous_day_death}')
print(f'One day after death = {one_day_after_death}')
print(f'difference = {one_day_after_death - previous_day_death}')

"""Hence we will impute '20/Apr/2025' deaths with mean of these two days as the death_data is a cummulative data."""

death_data.loc[nan_rows.index, '20/Apr/2020'] = (previous_day_death + one_day_after_death)/2
death_data.loc[nan_rows.index, '20/Apr/2020']

print(f' Number of Columns containing NaN values = {death_data.isnull().any().sum()}', end = '\n\n')

null_col = death_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""Now we can convert dtype of date columns from Object to int"""

date_columns = death_data.columns[4:]
death_data[date_columns] = death_data[date_columns].astype('int64')

death_data.info()

"""# **Analysis of recovered_case_data**"""

recovered_case_data = pd.read_csv('/content/drive/MyDrive/ChaiCode/covid_19_recovered_v1_lyst1747728719904.csv')

print(f'The shape of the confirmed_cases_data set is {recovered_case_data.shape}', end = '\n\n')
recovered_case_data.head(3)

"""Here also actual dataset would be 261x498

*We can see it has fomat anamolies similar to dataset 2.*

*So, perform all similar formatting options.*
"""

# Reset the column names using the first row
recovered_case_data.columns = recovered_case_data.iloc[0] # but we have not dropped the row1 which is column nmaes still.
# Drop the first row since it's now the header
recovered_case_data = recovered_case_data.drop(0).reset_index(drop=True) # reset index because you dropped first row. So, now index will be from 2.
#drop true means old index is removed and not added as a new column in your DataFram. Basically lost.

#converting dataframe columns to list columns to do swap
col_list = list(recovered_case_data.columns)
col_list[0], col_list[1] = col_list[1], col_list[0]
# assigning column list back to dataframe
recovered_case_data = recovered_case_data[col_list]

recovered_case_data.rename(columns={'Lat': 'Latitude', 'Long': 'Longitude'}, inplace=True)

#To convert them we need to parse them as date, then only we can reformat them.
#It also ensures all dates are correctly interpreted and formatted, especially if months/days are single digits.
date_columns = recovered_case_data.columns[4:]
new_date_columns = pd.to_datetime(date_columns, format='%m/%d/%y').strftime('%d/%b/%Y') ## strftime convert datetime object to a string in a specified format.
recovered_case_data.columns = list(recovered_case_data.columns[:4]) + list(new_date_columns)

recovered_case_data.head(3)

"""*Dtypes Correction*"""

pd.DataFrame(recovered_case_data.dtypes).T

"""*Converting latitude and longitude dtype to float*"""

recovered_case_data['Longitude'] = recovered_case_data['Longitude'].astype('float64')
recovered_case_data['Latitude'] = recovered_case_data['Latitude'].astype('float64')

"""*Check if date columns have dtype other than object*"""

print( * (col for col in recovered_case_data.columns[4:] if 'object' not in str(recovered_case_data[col].dtype)))

"""since all date columns are object, they can have Nan values which can result in Valueerror in conversion from object dtype to int64 dtype.

So, we will first remove Nan then only convert its dtype.
"""

print(f' Number of Columns containing NaN values = {recovered_case_data.isnull().any().sum()}', end = '\n\n')

null_col = recovered_case_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""*Imputing provinces with value 'All Provinces'*"""

recovered_case_data['Province/State'] = recovered_case_data['Province/State'].fillna('All Provinces')
recovered_case_data.head(3)

"""*Checking which rows has null or 0 latitudes and longitudes*"""

missing_coords = recovered_case_data[
    recovered_case_data['Latitude'].isnull() |
    (recovered_case_data['Latitude'] == 0) |
    recovered_case_data['Longitude'].isnull() |
    (recovered_case_data['Longitude'] == 0)
]
print(missing_coords.to_string())

"""We can see the situation is same as first dataset but only 3 rows this time.

*Imputing Latitudes & Longitudes.*
"""

for country in ['China']:
    country_specific_df = recovered_case_data['Country/Region'] == country
    # Calculate mean for non-zero, non-NaN latitudes and longitudes
    latitude_mean = recovered_case_data.loc[country_specific_df & (recovered_case_data['Latitude'] != 0), 'Latitude'].mean()
    longitude_mean = recovered_case_data.loc[country_specific_df & (recovered_case_data['Longitude'] != 0), 'Longitude'].mean()
    # Replace NaN or 0 with the mean
    empty_latitude_column = country_specific_df & (recovered_case_data['Latitude'].isnull() | (recovered_case_data['Latitude'] == 0))
    empty_longitude_column = country_specific_df & (recovered_case_data['Longitude'].isnull() | (recovered_case_data['Longitude'] == 0))
    recovered_case_data.loc[empty_latitude_column, 'Latitude'] = latitude_mean
    recovered_case_data.loc[empty_longitude_column, 'Longitude'] = longitude_mean

# to update specific latitude and longitude
recovered_case_data.loc[90, ['Latitude', 'Longitude']] = [180, -180]
recovered_case_data.loc[158, ['Latitude', 'Longitude']] = [180, -180]

missing_coords = recovered_case_data[
    recovered_case_data['Latitude'].isnull() |
    (recovered_case_data['Latitude'] == 0) |
    recovered_case_data['Longitude'].isnull() |
    (recovered_case_data['Longitude'] == 0)
]
if missing_coords.empty:
    print("No missing or zero coordinates found.", end = '\n\n')
else:
    print(missing_coords.to_string(), end = '\n\n')

recovered_case_data.loc[[73,90,158]] # loc prints the row as a Series by default. We have printed it as DataFrame row using [[]]

"""*Imputing column '20/Apr/2020' Nan value.*

Lets see how many rows of column '20/Apr/2025' has NaN value.
"""

nan_rows = recovered_case_data[recovered_case_data['20/Apr/2020'].isnull()] # returns dataframe of all rows with NaN in column 20/Apr/2020
# print(nan_rows.to_string()) # -> df
print(nan_rows.index, end = '\n\n') # returns indexes of rows having NaN values -> Index (labels to access values in series or df). print(nan_rows.index[0])
recovered_case_data.loc[nan_rows.index, '20/Apr/2020'] # .loc[row label, column lablel] -> series (1D labelled array and that single column index will be 0)

"""We can impute it with previous or forward day data, but lets see both days data first."""

previous_day_death = int(recovered_case_data.loc[nan_rows.index, '19/Apr/2020'].values[0])
one_day_after_death = int(recovered_case_data.loc[nan_rows.index, '21/Apr/2020'].values[0])

print(f'Previous day death = {previous_day_death}')
print(f'One day after death = {one_day_after_death}')
print(f'difference = {one_day_after_death - previous_day_death}')

"""Hence we will impute '20/Apr/2025' deaths with mean of these two days as the death_data is a cummulative data."""

recovered_case_data.loc[nan_rows.index, '20/Apr/2020'] = (previous_day_death + one_day_after_death)/2
recovered_case_data.loc[nan_rows.index, '20/Apr/2020']

"""`mean of the previous and next day's values—also known as interpolation.`
`recovered_case_data['20/Apr/2025'].interpolate(method='linear', inplace=True)`
"""

print(f' Number of Columns containing NaN values = {recovered_case_data.isnull().any().sum()}', end = '\n\n')

null_col = recovered_case_data.isnull().sum()
print(null_col [null_col>0].to_string())

"""Now we can convert dtype of date columns to int"""

date_columns = recovered_case_data.columns[4:]
recovered_case_data[date_columns] = recovered_case_data[date_columns].astype('int64')

recovered_case_data.info()

"""# ***Questions & Data Analysis***
---

Q1: ***How do you load the COVID-19 datasets for confirmed cases, deaths, and recoveries into Python using Pandas?***

Ans: Using readcsv() function of Panda'ss module, so that we can use pandas functionalities on our data set.

Q2.1: After loading the datasets, what is the structure of each dataset in terms of rows, columns, and data types?

Ans:
*   confirmed_case_data = (276, 498); {COuntry: Object, Province: Object, Latitude: Float, Longitude: FLoat, Date*: int64}
*   death_data = (277, 498); { col* : Object} later traformed to {Country: Object, Province: Object, Latitude: Float, Longitude: Float, Date*: int64}
*   recovered_case_data = (262, 498); { col* : Object} later traformed to {Country: Object, Province: Object, Latitude: Float, Longitude: Float, Date*: int64}

Q3.1: Identify these missing values and replace them using a suitable imputation method, such as forward filling, for time-series data.

Ans:
*   Provinces column missing values are imputed with 'All Provinces' value.
*   Latitude and Longitude values are imputed with mean of its country's values. And those for which there was single entry, (180,-180) is taken as special case & documented.

Q4.1: Replace blank values in the province column with "All Provinces."

And: Done

Q2.2: Generate plots of confirmed cases over time for the top countries.

Ans: The data is cummulative, so last day will give us total confirmed cases.
"""

country_based_data = confirmed_case_data.groupby('Country/Region').sum()
last_day_cases = country_based_data.loc[: , country_based_data.columns[-1]] # row,column
top_cases = last_day_cases.sort_values(ascending = False).head(10)
top_cases

# lineplot has categorical x axis. Trends over continuous x-axis; less intuitive for categories.
fig1 = plt.figure(figsize=(8, 3.5))
ax = sns.lineplot(data = top_cases, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
plt.xlabel('Country')
plt.ylabel('Confirmed Cases')
plt.title('Top 10 Countries with Highest Confirmed Cases')
plt.xticks(rotation=45)
   # To avoid exponential notation on y-axis
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
   #"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.show()
plt.close()

#Comparing discrete categories (like countries)
fig2 = plt.figure(figsize=(8,3.5))
ax = sns.barplot(x = top_cases.index, y = top_cases.values, palette='Blues')
plt.xlabel('Country')
plt.ylabel('Confirmed Cases')
plt.title('Top 10 Countries with Highest Confirmed Cases')
plt.xticks(rotation=45)
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
plt.show()
plt.close()

"""Q2.3: Generate plots of confirmed cases over time for China."""

china_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'China']
china_cases_series = china_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
china_cases_series # series indexed by dates. need to covert to dataframe for transpose

# converting china_cases_series.index to datetime format
china_cases_series.index = pd.to_datetime(china_cases_series.index)

# Now group by month and sum values
china_monthly = china_cases_series.groupby(china_cases_series.index.strftime("%b/%Y")).sum()
# this sorts grouped series in alphabetical order (e.g., "Apr" comes before "Aug" and "Dec") instead of chronological order.
# to sort inchronological order we need to convert back to %m%Y and then sort and then convert back to %b%Y.

#Converting to %m -> sorting -> back to %b

# Convert index to datetime, then format it as 'MM/YYYY'
china_monthly.index = pd.to_datetime(china_monthly.index, format='%b/%Y')

# Sort the index
china_monthly = china_monthly.sort_index()

# Convert index back to 'Mon/YYYY'
china_monthly.index = pd.to_datetime(china_monthly.index, format='%m/%Y').strftime('%b/%Y')

china_monthly

plt.clf()
  # lineplot has categorical x axis.
fig3 = plt.figure(figsize=(10,5))
axo = sns.lineplot(data = china_monthly, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
axo.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
#"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.xlabel('Days')
plt.ylabel('Confirmed Cases per month')
plt.title('China cases per Month')
plt.xticks(rotation=45)
   # To avoid exponential notation on y-axis
plt.savefig('china_cases_per_day.png')
plt.show()

# if you are giving two times sns.lineplot to use data in one and attributes in another, it will lead next grapgh to catch first graph data.

"""Q5.1: Analyze the peak number of daily new cases in Germany, France, and Italy. Which country experienced the highest single-day surge, and when did it occur?

Ans: Approach: Group data for these 3 countries. Then find the maximum different between consecutive days. Then plot graph of all 3 countries.
"""

germany_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Germany']
germany_daily_series = germany_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
germany_daily_series

france_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'France']
france_daily_series = france_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
france_daily_series

italy_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Italy']
italy_daily_series = italy_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
italy_daily_series

# italy_daily_series is a series. Accessing using loc, loc needs actual label which is date here. But iloc works with default index which are 0,1, etc.
# italy_daily_series.index[0] will give 0th row label.

series_dict = {
    "Germany": germany_daily_series,
    "France": france_daily_series,
    "Italy": italy_daily_series
}

max_diff_dict = {}

for name, seri in series_dict.items():
    max_diff = 0
    cases_surge_date = None

    for idx in range(1, len(seri)):  # Iterate through entire Series
        diff = seri.iloc[idx] - seri.iloc[idx - 1]
        if diff > max_diff:
            max_diff = diff
            cases_surge_date = seri.index[idx]

    print(f"{name}: Greatest difference is {max_diff} on {cases_surge_date}")
    max_diff_dict[name] = (int(max_diff), cases_surge_date)

print('\n the max_diff_dict is')
max_diff_dict

# Extract labels and values
labels = list(max_diff_dict.keys())  # Country names
proportion = [cases[0] for cases in max_diff_dict.values()]  # Case numbers
dates = [date[1] for date in max_diff_dict.values()]  # Surge dates

print(labels)
print(proportion)
print(dates)

# Custom function to display percentage, value, and date
# Custom function to display proportion and date
def autopct_format(all_values, all_dates):
    def format_func(pct):
        total = sum(all_values)
        absolute = int(round(pct * total / 100.0))  # Convert percentage to absolute value
        index = next((i for i, v in enumerate(all_values) if v == absolute), None)  # Find correct index
        if index is not None:
            return f"{all_values[index]}\n{all_dates[index]}"  # Display case numbers & date
        return ""

    return format_func

# Create Pie Chart
fig4 = plt.figure(figsize=(5,5))
plt.pie(proportion, labels=labels, autopct=autopct_format(proportion, dates), startangle=140, colors=['lightblue', 'lightcoral', 'lightgreen'])
plt.title("COVID-19 Case Surge Distribution")
plt.show()

"""Q6.2: What is the total number of deaths reported per country up to the current date?"""

country_based_death_data = death_data.groupby('Country/Region').sum()
last_day_deaths = country_based_death_data.iloc[:, -1]  # Use iloc for last column selection
top_deaths_countries = last_day_deaths.sort_values(ascending=False).head(10)
top_deaths_countries

#Comparing discrete categories (like countries)
fig5 = plt.figure(figsize=(8,3.5))
ax = sns.barplot(x = top_deaths_countries.index, y = top_deaths_countries.values, palette='Blues')
plt.xlabel('Country')
plt.ylabel('Total Death Cases')
plt.title('Top 10 Countries with Highest Death Cases')
plt.xticks(rotation=45)
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
plt.show()
plt.close()

"""Q6.3: What are the top 5 countries with the highest average daily deaths?"""

country_based_death_data = death_data.groupby('Country/Region').sum()
last_day_deaths = country_based_death_data.iloc[:, -1]  # Use iloc for last column selection
top_deaths_countries = last_day_deaths.sort_values(ascending=False).head(5)

date_columns = death_data.columns[4:]

top_avg_deaths_countries = top_deaths_countries / (len(date_columns))
top_avg_deaths_countries

"""Q5.3: What is the distribution of death rates (deaths/confirmed cases) among provinces in Canada? Identify the province with the highest and lowest death
rate as of the latest data point.

Ans: Approach:
`Confirmed case data -> pick only Canada as country with its provinces`

`Death case data ->   pick only Canada as country with its provinces`

`Do dividion for death rate ->  Confirmed case data / Death case data`

`Plot`
"""

canada_confirmed_case = confirmed_case_data[confirmed_case_data['Country/Region'] == "Canada"]
canada_confirmed_case_data = canada_confirmed_case.drop(columns=['Country/Region', 'Latitude', 'Longitude']).set_index('Province/State').iloc[:,-1]

canada_death_case = death_data[death_data['Country/Region'] == "Canada"]
canada_death_case_data = canada_death_case.drop(columns=['Country/Region', 'Latitude', 'Longitude']).set_index('Province/State').iloc[:,-1]
canada_death_case_data

canada_death_rate = (canada_death_case_data/canada_confirmed_case_data)*100
canada_death_rate

# it will inf only if 0/any means no case but died. Or 0/0 means no case no death.
# Impute inf values with 0
canada_death_rate = canada_death_rate.replace([np.inf, -np.inf], 0)
print(canada_death_rate)  #Ex For Alberta = 2214 deaths/226855cases = 0.97%

fig6 = plt.figure(figsize=(8, 6))
ax = sns.lineplot(data = canada_death_rate, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
plt.xlabel('Canada Provinces')
plt.ylabel('Death rates')
plt.title('Canada province death rates')
plt.xticks(rotation=90)
   # To avoid exponential notation on y-axis
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
   #"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
# Add number annotations at each marker
for i, value in enumerate(canada_death_rate):
    ax.text(i, value, f"{value:.2f}", fontsize=10, ha='center', va='bottom', color='black')
plt.show()
plt.close()

"""Q6.4: How have the total deaths evolved over time in the United States?

Ans: Actually it is asking trend of deaths in US overtime. So, we will plot for month wise deaths for US and see if the deaths increased or decresed (i.e., evolution)
"""

US_data = death_data[death_data['Country/Region'] == 'US']
US_cases_series = US_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
US_cases_series # series indexed by dates. need to covert to dataframe for transpose

# converting US_cases_series.index to datetime format
US_cases_series.index = pd.to_datetime(US_cases_series.index)

# Now group by month and sum values
US_monthly = US_cases_series.groupby(US_cases_series.index.strftime("%b/%Y")).sum()
# this sorts grouped series in alphabetical order (e.g., "Apr" comes before "Aug" and "Dec") instead of chronological order.
# to sort inchronological order we need to convert back to %m%Y and then sort and then convert back to %b%Y.

#Converting to %m -> sorting -> back to %b

# Convert index to datetime, then format it as 'MM/YYYY'
US_monthly.index = pd.to_datetime(US_monthly.index, format='%b/%Y')

# Sort the index
US_monthly = US_monthly.sort_index()

# Convert index back to 'Mon/YYYY'
US_monthly.index = pd.to_datetime(US_monthly.index, format='%m/%Y').strftime('%b/%Y')

US_monthly

plt.clf()
  # lineplot has categorical x axis.
fig7 = plt.figure(figsize=(10,5))
axo = sns.lineplot(data = US_monthly, marker='o', markerfacecolor='blue', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
axo.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
#"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.xlabel('Days')
plt.ylabel('Confirmed Cases per month')
plt.title('China cases per Month')
plt.xticks(rotation=45)
   # To avoid exponential notation on y-axis
plt.savefig('china_cases_per_day.png')
plt.show()

# if you are giving two times sns.lineplot to use data in one and attributes in another, it will lead next grapgh to catch first graph data.

"""Q5.2: Compare the recovery rates (recoveries/confirmed cases) between Canada and Australia as of December 31, 2020. Which country showed better management of the
pandemic according to this metric?
"""

#Canada positive data
canada_confirm_case_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Canada']
canada_confirm_case_data_series = canada_confirm_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
#sums column-wise means for each date column, it sums across all provinces in Canada.
canada_case_31Dec2020 = canada_confirm_case_data_series.loc['31/Dec/2020']
print(f'canada confirmed case on 31dec 2020 is {canada_case_31Dec2020}')

#Canada recovery data
canada_recovery_case_data = recovered_case_data[recovered_case_data['Country/Region'] == 'Canada']
canada_recovery_case_data_series = canada_recovery_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
canada_recovery_case_31Dec2020 = canada_recovery_case_data_series.loc['31/Dec/2020']
print(f'canada confirmed case on 31dec 2020 is {canada_recovery_case_31Dec2020}')

canada_recovery_rate = (canada_recovery_case_31Dec2020/canada_case_31Dec2020)*100
print(f'canada recovery rate is {canada_recovery_rate}')

#Australia positive data
australia_confirm_case_data = confirmed_case_data[confirmed_case_data['Country/Region'] == 'Australia']
australia_confirm_case_data_series = australia_confirm_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
#sums column-wise means for each date column, it sums across all provinces in Australia.
australia_case_31Dec2020 = australia_confirm_case_data_series.loc['31/Dec/2020']
print(f'Australia confirmed case on 31dec 2020 is {australia_case_31Dec2020}')

#Australia recovery data
australia_recovery_case_data = recovered_case_data[recovered_case_data['Country/Region'] == 'Australia']
australia_recovery_case_data_series = australia_recovery_case_data.drop(columns=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum()
australia_recovery_case_31Dec2020 = australia_recovery_case_data_series.loc['31/Dec/2020']
print(f'Australia confirmed case on 31dec 2020 is {australia_recovery_case_31Dec2020}')

australia_recovery_rate = (australia_recovery_case_31Dec2020/australia_case_31Dec2020)*100
print(f'australia recovery rate is {australia_recovery_rate}')

#Comparing discrete categories (like countries)
fig8 = plt.figure(figsize=(2,4))
ax = sns.barplot(x = ['Canada', 'Australis'], y = [canada_recovery_rate, australia_recovery_rate], palette='Greens')
plt.xlabel('Country')
plt.ylabel('Recovery rate')
plt.title('Recovery rate on 31dec/2020')
plt.xticks(rotation=45)
ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
plt.show()
plt.close()

"""Hence we can see Canada government managed well for the recovery from the pendamic for day 31 Dec 2020 from the given metrics.

# **Merging of Data sets and Analysis**

Q6.1: Transform the 'deaths' dataset from wide format (where each column represents a date) to long format, where each row represents a single date,
ensuring that the date column is in datetime format. How would this transformation be executed?

Ans: Wide form (where each date is a separate column). Lonf form long format (where all dates are in a single "Date" column). So for each country, province, long, lat there will be 494 rows (date columns count in original df). ANd there are 276 rows so 276*(498-4) = 13,63,44 total rows will come in long form. We are not performing any column wise province sum here.

This transformation makes time-series analysis much easier.
"""

deaths_data_long = pd.melt(
    death_data,
    id_vars=['Country/Region', 'Province/State', 'Latitude', 'Longitude'], # remains unchanged
    var_name='Date',
    value_name='Deaths' # values that were originally spread across multiple date columns
)
deaths_data_long['Date'] = pd.to_datetime(deaths_data_long['Date'], format='%d/%b/%Y')
print(f'shape of deaths {death_data.shape}')
print(f'deaths_data_long.shape {deaths_data_long.shape}', end = '\n\n')

deaths_data_long

"""If you are getting less number of rows after tranforming into long form, here are few checks to be done. Note that pd.melt() drops rows if there are any NaN values, or the column names contains white spaces. But it does not drop rows for 0 values."""

# IGNORE (NOT NEEDED ANYMORE)

# deaths_data_long['Date'].unique().tolist() All date are in same & correct format also. Try converting your dates to datetimeformat.
# death_data.duplicated().sum() No duplicates also.
print("Expected Row Count:", 277 * (498 - 4))  # (subtracting non-date columns)
print("Actual Row Count:", deaths_data_long.shape[0])

print("Unique Dates in Long Data:", deaths_data_long['Date'].nunique())
print("Total Expected Unique Dates:", 498 - 4)  # Excluding country, province, lat, long

# so issue is not with dates. It is elsewhere.
print("Duplicate rows:", death_data.duplicated(subset=['Country/Region', 'Province/State', 'Latitude', 'Longitude']).sum())
print(death_data.columns.tolist(), end = '\n\n')  # Check for unexpected column names

date_cols = death_data.columns.difference(['Country/Region', 'Province/State', 'Latitude', 'Longitude'])
print(len(date_cols))  # Should be 494
all_zero_rows = (death_data[date_cols] == 0).all(axis=1).sum()
print("Rows with all zeros:", all_zero_rows)

original_keys = set(zip(death_data['Country/Region'], death_data['Province/State'], death_data['Latitude'], death_data['Longitude']))
melted_keys = set(zip(deaths_data_long['Country/Region'], deaths_data_long['Province/State'], deaths_data_long['Latitude'], deaths_data_long['Longitude']))
print("Rows missing after melt:", original_keys - melted_keys)

"""Q7.1: How would you merge the transformed datasets of confirmed cases, deaths, and recoveries on the 'Country/Region' and 'Date' columns to create a
comprehensive view of the pandemic's impact?

Ans: First we will convert other 2 dataset to long form and that data we will convert it to at the country and date level, you should aggregate your data so that each row represents the total deaths for a country on a specific date.
"""

confirmed_case_data_long = pd.melt(
    confirmed_case_data, # original dataset name
    id_vars=['Country/Region', 'Province/State', 'Latitude', 'Longitude'], # remains unchanged
    var_name='Date',
    value_name='confirmed_cases' # values that were originally spread across multiple date columns
)
confirmed_case_data_long['Date'] = pd.to_datetime(confirmed_case_data_long['Date'], format='%d/%b/%Y')
print(f'shape of confirmed case {confirmed_case_data.shape}')
print(f'confirmed_case_data_long.shape {confirmed_case_data_long.shape}', end = '\n\n')

recovered_case_data_long = pd.melt(
    recovered_case_data,
    id_vars=['Country/Region', 'Province/State', 'Latitude', 'Longitude'], # remains unchanged
    var_name='Date',
    value_name='recovered_cases' # values that were originally spread across multiple date columns
)
recovered_case_data_long['Date'] = pd.to_datetime(recovered_case_data_long['Date'], format='%d/%b/%Y')
print(f'shape of recovered case {recovered_case_data.shape}')
print(f'recovered_case_data_long.shape {recovered_case_data_long.shape}')

# This gives you the total deaths per country "per day", regardless of province/state. Summing for per day.
country_date_confirmed = confirmed_case_data_long.groupby(['Country/Region', 'Date'])['confirmed_cases'].sum().reset_index()
print(f'shape of country_date_confirmed is {country_date_confirmed.shape}')

country_date_deaths = deaths_data_long.groupby(['Country/Region', 'Date'])['Deaths'].sum().reset_index()
print(f'shape of country_date_deaths is {country_date_deaths.shape}')

country_date_revovered = recovered_case_data_long.groupby(['Country/Region', 'Date'])['recovered_cases'].sum().reset_index()
print(f'shape of country_date_revovered is {country_date_revovered.shape}', end='\n\n')
country_date_revovered

"""You can see we first group by country+date based and then merging all 3 datasets. Using this approach we got same rows, which we could not get if we were first merging and then grouping it by country+date"""

# Merge confirmed and deaths
merged = pd.merge(
    country_date_confirmed,
    country_date_deaths,
    on=['Country/Region', 'Date'],
    how='outer'       # how='outer' ensures you keep all country-date combinations, even if one DataFrame is missing a value for that pair
)

# Merge with recovered
dataset_merged_country_date  = pd.merge(
    merged,
    country_date_revovered,
    on=['Country/Region', 'Date'],
    how='outer'
)
print(f'shape of dataset_merged_country_date is {dataset_merged_country_date.shape}')
dataset_merged_country_date

"""This merged DataFrame now provides a comprehensive, per-country per-day view of the pandemic’s impact and is ready for further analysis or visualization.

*Q7.2: Analyze the monthly sum of confirmed cases, deaths, and recoveries for countries to understand the progression of the pandemic.[From the merged dataset]*
"""

# To see progression let us have more columns
#Ensure the DataFrame is sorted by Country/Region and Date
dataset_merged_country_date_extended = dataset_merged_country_date.sort_values(['Country/Region', 'Date'])

# Calculate daily new cases by group
dataset_merged_country_date_extended['unique_confirmed_case'] = dataset_merged_country_date_extended.groupby('Country/Region')['confirmed_cases'].diff().fillna(dataset_merged_country_date_extended['confirmed_cases'])
dataset_merged_country_date_extended['unique_death_case'] = dataset_merged_country_date_extended.groupby('Country/Region')['Deaths'].diff().fillna(dataset_merged_country_date_extended['Deaths'])
dataset_merged_country_date_extended['unique_recovered_case'] = dataset_merged_country_date_extended.groupby('Country/Region')['recovered_cases'].diff().fillna(dataset_merged_country_date_extended['recovered_cases'])

# ['confirmed_cases'].diff() calculates the difference between the current and previous row within each group.
# .fillna(df['confirmed_cases']) The first row for each country will have NaN because there's no previous day to compare. Here fillna replaces that NaN with the original confirmed case count, ensuring no missing values.
dataset_merged_country_date_extended

# converting 'Date' column to datetime format
dataset_merged_country_date_extended['Date'] = pd.to_datetime(dataset_merged_country_date_extended['Date'])

# extracted year and month to group by and created new column
dataset_merged_country_date_extended['YearMonth'] = dataset_merged_country_date_extended['Date'].dt.to_period('M')  #.dt → Enables direct access to datetime properties.
#.to_period('M') → Converts each date into a year-month period (YYYY-MM format)

# Now group by country+year-month and do perform sum.
merged_dataset_monthly_summary = dataset_merged_country_date_extended.groupby(['Country/Region', 'YearMonth'])[['confirmed_cases', 'Deaths', 'recovered_cases', 'unique_confirmed_case', 'unique_death_case', 'unique_recovered_case']].sum().reset_index()
# Groups the data by Country/Region and YearMonth.
# [['confirmed_cases', 'Deaths', 'recovered_cases']].sum()  After grouping, it calculates the monthly sum of these 3 columns.
# Converts the grouped data back into a regular DataFrame (instead of keeping it as a Pandas groupby object).
print(f'shape of merged_dataset_monthly_summary on country+dates basis is {merged_dataset_monthly_summary.shape}', end = '\n\n')

# we cannot see the Date column beacuse we may have directly assigned mon-year col to date col of df. dataset_merged_country_date_extended['Date'] = dataset_merged_country_date_extended['Date'].dt.to_period('M')  # Overwrites Date
# Here we have done .groupby(['Country/Region', 'YearMonth']), Pandas automatically removes non-grouped columns (like Date) from the result unless explicitly retained.
merged_dataset_monthly_summary

"""lets use above data to see India trends"""

india_monthly = merged_dataset_monthly_summary[merged_dataset_monthly_summary['Country/Region'] == 'India']
# Ensure YearMonth is string or datetime for plotting
india_monthly = india_monthly.sort_values('YearMonth')
india_monthly['YearMonth'] = india_monthly['YearMonth'].astype(str)

fig9 = plt.figure(figsize=(12, 6))
ax = sns.lineplot(
    x='YearMonth', y='confirmed_cases',
    data=india_monthly,
    label='Confirmed', color='orange'
)
sns.lineplot(
    x='YearMonth', y='Deaths',
    data=india_monthly,
    label='Deaths', color='red'
)
sns.lineplot(
    x='YearMonth', y='recovered_cases',
    data=india_monthly,
    label='Recovered', color='green'
)
plt.xlabel('Month-Year')
plt.ylabel('Monthly Total')
plt.title('Monthly COVID-19 Cases in India')
plt.xticks(rotation=45)
plt.legend() #creates a box—usually in a corner of the plot to display labels
plt.tight_layout()
ax.grid(False)  # to remove grids.
plt.show()

india_monthly = merged_dataset_monthly_summary[merged_dataset_monthly_summary['Country/Region'] == 'India']
# Ensure YearMonth is string or datetime for plotting
india_monthly = india_monthly.sort_values('YearMonth')
india_monthly['YearMonth'] = india_monthly['YearMonth'].astype(str)

fig10 = plt.figure(figsize=(12, 6))
ax = sns.lineplot(
    x='YearMonth', y='unique_confirmed_case',
    data=india_monthly,
    label='unique_confirmed_case', color='orange'
)
sns.lineplot(
    x='YearMonth', y='unique_death_case',
    data=india_monthly,
    label='unique_death_case', color='red'
)
sns.lineplot(
    x='YearMonth', y='unique_recovered_case',
    data=india_monthly,
    label='unique_recovered_case', color='green'
)
plt.xlabel('Month-Year')
plt.ylabel('Monthly Total')
plt.title('Monthly "unique" COVID-19 Cases in India')
plt.xticks(rotation=45)
plt.legend() #creates a box—usually in a corner of the plot to display labels
plt.tight_layout()
ax.grid(False)  # to remove grids.
plt.show()

"""Q7.3: Redo the analysis in Question 7.2 for the United States, Italy, and Brazil."""

US_monthly = merged_dataset_monthly_summary[merged_dataset_monthly_summary['Country/Region'] == 'US']
# Ensure YearMonth is string or datetime for plotting
US_monthly = US_monthly.sort_values('YearMonth')
US_monthly['YearMonth'] = US_monthly['YearMonth'].astype(str)

fig11 = plt.figure(figsize=(12, 6))
ax = sns.lineplot(
    x='YearMonth', y='confirmed_cases',
    data=US_monthly,
    label='Confirmed', color='orange'
)
sns.lineplot(
    x='YearMonth', y='Deaths',
    data=US_monthly,
    label='Deaths', color='red'
)
sns.lineplot(
    x='YearMonth', y='recovered_cases',
    data=US_monthly,
    label='Recovered', color='green'
)
plt.xlabel('Month-Year')
plt.ylabel('Monthly Total')
plt.title('Monthly COVID-19 Cases in US')
plt.xticks(rotation=45)
plt.legend() #creates a box—usually in a corner of the plot to display labels
plt.tight_layout()
ax.grid(False)  # to remove grids.
plt.show()

Italy_monthly = merged_dataset_monthly_summary[merged_dataset_monthly_summary['Country/Region'] == 'Italy']
# Ensure YearMonth is string or datetime for plotting
Italy_monthly = Italy_monthly.sort_values('YearMonth')
Italy_monthly['YearMonth'] = Italy_monthly['YearMonth'].astype(str)

fig12 = plt.figure(figsize=(12, 6))
ax = sns.lineplot(
    x='YearMonth', y='confirmed_cases',
    data=Italy_monthly,
    label='Confirmed', color='orange'
)
sns.lineplot(
    x='YearMonth', y='Deaths',
    data=Italy_monthly,
    label='Deaths', color='red'
)
sns.lineplot(
    x='YearMonth', y='recovered_cases',
    data=Italy_monthly,
    label='Recovered', color='green'
)
plt.xlabel('Month-Year')
plt.ylabel('Monthly Total')
plt.title('Monthly COVID-19 Cases in Italy')
plt.xticks(rotation=45)
plt.legend() #creates a box—usually in a corner of the plot to display labels
plt.tight_layout()
ax.grid(False)  # to remove grids.
plt.show()

brazil_monthly = merged_dataset_monthly_summary[merged_dataset_monthly_summary['Country/Region'] == 'Brazil']
# Ensure YearMonth is string or datetime for plotting
brazil_monthly = brazil_monthly.sort_values('YearMonth')
brazil_monthly['YearMonth'] = brazil_monthly['YearMonth'].astype(str)

fig13 = plt.figure(figsize=(12, 6))
ax = sns.lineplot(
    x='YearMonth', y='confirmed_cases',
    data=brazil_monthly,
    label='Confirmed', color='orange'
)
sns.lineplot(
    x='YearMonth', y='Deaths',
    data=brazil_monthly,
    label='Deaths', color='red'
)
sns.lineplot(
    x='YearMonth', y='recovered_cases',
    data=brazil_monthly,
    label='Recovered', color='green'
)
plt.xlabel('Month-Year')
plt.ylabel('Monthly Total')
plt.title('Monthly COVID-19 Cases in Brazil')
plt.xticks(rotation=45)
plt.legend() #creates a box—usually in a corner of the plot to display labels
plt.tight_layout()
ax.grid(False)  # to remove grids.
plt.show()

"""We can infer that in Brazil the COVID cases took spike from April month. But for US & Italy it was from march only.

Whereas for India it was from early June.

This shows India had better mitigatation processin place than these countries.

*Q8.1: For the combined dataset, identify the three countries with the highest average death rates (deaths/confirmed cases) throughout 2020. What might this indicate about the pandemic's impact in these countries?*

Approach: Extract 2020 data -> Find how many unique dates are there because we have to find average at last -> then do group by countries and sum for last three columns -> then do death/confirmed_cases which should result in a series with death rates column and labels as countries.
"""

print('We have merged dataset named merged_dataset_monthly_summary')
merged_dataset_monthly_summary

#filter 2020 data
merged_dataset_monthly_summary_2020 = merged_dataset_monthly_summary[merged_dataset_monthly_summary['YearMonth'].astype(str).str.startswith('2020')]

#count number of months in 2020. nunique() Counts the number of distinct values in that column
num_unique_dates = merged_dataset_monthly_summary_2020['YearMonth'].nunique()
print(f"Number of unique YearMonth in 2020: {num_unique_dates}")

country_summary_2020 = merged_dataset_monthly_summary_2020.groupby('Country/Region')[['confirmed_cases', 'Deaths', 'recovered_cases']].sum()

# getting death rates.
death_rate_2020 = (country_summary_2020['Deaths'] / country_summary_2020['confirmed_cases'])*100
death_rate_2020.name = 'death_rate'

#sort in descending order and get top 3 countries having max death rates.
death_rate_2020_top3 = death_rate_2020.sort_values(ascending=False).head(3)
death_rate_2020_top3

fig14 = plt.figure(figsize=(4,4))
death_rate_2020_top3.plot(
    kind='pie',
    autopct='%1.2f%%',    # Show percentage with two decimals
    startangle=90,        # where the first slice of the pie chart begins, measured in degrees counterclockwise from the positive x-axis
    colors=['lightcoral', 'lightskyblue', 'lightgreen'] # Optional: custom colors
)
plt.title('Top 3 Countries by COVID-19 Death Rate (2020)')
plt.ylabel('')  # Hide y-label for aesthetics
plt.tight_layout()
plt.show()

"""Q8.2: Using the merged dataset, compare the total number of recoveries to the total number of deaths in South Africa. What can this tell us about the outcomes of COVID-19 cases in the country?

Ans: It is asking total number of recoveries and deaths, which will be present in last day entry as it is cumulative data. Approach: extract Africa data from date wise merged data -> extract last date -> sum
"""

dataset_merged_country_date_extended.head(3)

"""from excel we observed Africa we don't have direct name of country. Hence filtered."""

africa_data = dataset_merged_country_date_extended[dataset_merged_country_date_extended['Country/Region'].str.contains('Africa', case=False)]
africa_data

africa_data_sorted = africa_data.sort_values('Date')
last_date = africa_data_sorted['Date'].max()
print(f'last date is {last_date}')

# Filter rows for last date as that will only be final value for differnt province.
africa_last_date_rows = africa_data_sorted[africa_data_sorted['Date'] == last_date]
print(africa_last_date_rows.to_string(), end = '\n\n') #index must be the row numbers.

columns_to_sum = ['confirmed_cases', 'Deaths', 'recovered_cases','unique_confirmed_case', 'unique_death_case', 'unique_recovered_case']
africa_totals = africa_last_date_rows[columns_to_sum].sum()
africa_totals

africa_totals = {
    'Confirmed Cases': 1666155.0,
    'Recovered Cases': 1560849.0,
    'Deaths': 56461.0
}
# Convert dictionary to a DataFrame
df = pd.DataFrame(list(africa_totals.items()), columns=['Category', 'Total'])

sns.set_style("whitegrid")
fig15 = plt.figure(figsize=(4, 4))
sns.barplot(x='Category', y='Total', data=df, palette='Greens')
plt.xlabel("Category")
plt.ylabel("Total Count")
plt.title("COVID-19 Cases Summary in Africa")
plt.xticks(rotation=45)  # Rotate labels for readability
plt.show()

"""Q8.3: Analyze the ratio of recoveries to confirmed cases for the United States monthly from March 2020 to May 2021. Which month experienced the highest recovery ratio, and what could be the potential reasons?

Ans: Approach: extract the data for US only of months from March 2020 and May2021 -> get last dated row of each month ->
"""

dataset_merged_country_date_extended

# Convert 'Date' column to datetime if not already; to ensure correct date filtering
dataset_merged_country_date_extended['Date'] = pd.to_datetime(dataset_merged_country_date_extended['Date'])

#filtering for US data for given months.
us_data = dataset_merged_country_date_extended[
    (dataset_merged_country_date_extended['Country/Region'] == 'US') &
    (dataset_merged_country_date_extended['Date'] >= '2020-03-01') &
    (dataset_merged_country_date_extended['Date'] <= '2021-05-31')
]

# Select only rows where the date is the last day of the month
us_month_last_date = us_data.loc[us_data.groupby([us_data['Date'].dt.year, us_data['Date'].dt.month])['Date'].idxmax()]
# or could have used us_month_last = us_data.loc[us_data.groupby('YearMonth')['Date'].idxmax()]

us_month_last_date

recovery_ratio = (us_month_last_date['recovered_cases'] / us_month_last_date['confirmed_cases'])*100
recovery_ratio.index = us_month_last_date['Date']
recovery_ratio
# below is for per day unique case recovery ratio
# recover_data excel sheet - row 239 13dec 14dec 2020 see data. so 0/any is 0 so recovery ratio 0.

fig16 = plt.figure(figsize=(8, 6))
ax1 = sns.lineplot(data = recovery_ratio, marker='o', markerfacecolor='red', markersize=6) # Refers to the axes object of your plot, which contains the x and y axes.
plt.xlabel('Month end dates')
plt.ylabel('Recovery ratio')
plt.title('Monthly recovery ratio for US')
plt.xticks(rotation=90)
   # To avoid exponential notation on y-axis
ax1.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x)))) # x: The value of the tick.
   #"{:,}".format(int(x)): Formats the tick value as an integer with commas as thousand separators (e.g., 10000 becomes "10,000")
plt.show()

"""`***THANK-YOU!***`

*`Regards`*

*`Pratik.`*
"""